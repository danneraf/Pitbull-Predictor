{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69a832e-0d94-49c7-9e7d-0f530fcd83ea",
   "metadata": {},
   "source": [
    "# Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6859e7-d4f4-43d1-a547-9999a42210f5",
   "metadata": {},
   "source": [
    "# been there done that "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed8c6e-35a5-48b6-9c9a-2d6ee495a202",
   "metadata": {},
   "source": [
    "![been there](../Images/beenthere.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d2166ff-39a2-49c7-be72-c106ae22b328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 02:34:21.513478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import torch \n",
    "from torch import cuda\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm \n",
    "\n",
    "from keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e86127-262b-44e5-b0b2-e1451afad710",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.read_csv('../Data/combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd4fbcc1-0ece-43b6-b49e-b38552e28cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_names</th>\n",
       "      <th>full_title</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>title_with_featured</th>\n",
       "      <th>url</th>\n",
       "      <th>featured_artists</th>\n",
       "      <th>artist</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "      <th>clean_cleaned_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pitbull (Ft. Vein (Producer))</td>\n",
       "      <td>11:59 by Pitbull (Ft. Vein (Producer))</td>\n",
       "      <td>654086</td>\n",
       "      <td>11:59</td>\n",
       "      <td>11:59 (Ft. Vein (Producer))</td>\n",
       "      <td>https://genius.com/Pitbull-11-59-lyrics</td>\n",
       "      <td>[{'api_path': '/artists/7076', 'header_image_u...</td>\n",
       "      <td>1</td>\n",
       "      <td>11 Contributors11:59 Lyrics[Intro: Pitbull &amp; V...</td>\n",
       "      <td>\\nWe shut down Times Square, extra extra read ...</td>\n",
       "      <td>['', 'pitbull', 'vein', 'vein', 'someone', 'po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pitbull (Ft. Lil Jon)</td>\n",
       "      <td>305 Anthem by Pitbull (Ft. Lil Jon)</td>\n",
       "      <td>37215</td>\n",
       "      <td>305 Anthem</td>\n",
       "      <td>305 Anthem (Ft. Lil Jon)</td>\n",
       "      <td>https://genius.com/Pitbull-305-anthem-lyrics</td>\n",
       "      <td>[{'api_path': '/artists/107', 'header_image_ur...</td>\n",
       "      <td>1</td>\n",
       "      <td>17 Contributors305 Anthem Lyrics[Intro: Pitbul...</td>\n",
       "      <td>\\nMan I've been on the grind\\n1 in the head, 1...</td>\n",
       "      <td>['', 'anthem', 'pitbull', 'lil', 'chico', 'pit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pitbull (Ft. Jamie Drastik)</td>\n",
       "      <td>305 In Paris by Pitbull (Ft. Jamie Drastik)</td>\n",
       "      <td>9071119</td>\n",
       "      <td>305 In Paris</td>\n",
       "      <td>305 In Paris (Ft. Jamie Drastik)</td>\n",
       "      <td>https://genius.com/Pitbull-305-in-paris-lyrics</td>\n",
       "      <td>[{'api_path': '/artists/26923', 'header_image_...</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Contributor305 In Paris Lyrics[Intro]\\nWe're...</td>\n",
       "      <td>[Chorus: Pitbull]\\nFlow so hard that chico get...</td>\n",
       "      <td>['', 'paris', 'gonna', 'skate', 'one', 'song',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pitbull</td>\n",
       "      <td>305 Til I Die by Pitbull</td>\n",
       "      <td>8988638</td>\n",
       "      <td>305 Til I Die</td>\n",
       "      <td>305 Til I Die</td>\n",
       "      <td>https://genius.com/Pitbull-305-til-i-die-lyrics</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Contributor305 Til I Die Lyrics[Intro]\\nIntr...</td>\n",
       "      <td>0</td>\n",
       "      <td>['', 'til', 'die', 'mr', 'three', 'yeah', 'yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Los Rafas (Ft. Pitbull)</td>\n",
       "      <td>3bkzo RIMIX by Los Rafas (Ft. Pitbull)</td>\n",
       "      <td>3972879</td>\n",
       "      <td>3bkzo RIMIX</td>\n",
       "      <td>3bkzo RIMIX (Ft. Pitbull)</td>\n",
       "      <td>https://genius.com/Los-rafas-3bkzo-rimix-lyrics</td>\n",
       "      <td>[{'api_path': '/artists/628', 'header_image_ur...</td>\n",
       "      <td>1</td>\n",
       "      <td>1 Contributor3bkzo RIMIX Lyrics[INTRO]\\nMateín...</td>\n",
       "      <td>0</td>\n",
       "      <td>['', 'rimix', 'mateín', 'checho', 'representan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    artist_names                                   full_title  \\\n",
       "0  Pitbull (Ft. Vein (Producer))       11:59 by Pitbull (Ft. Vein (Producer))   \n",
       "1          Pitbull (Ft. Lil Jon)          305 Anthem by Pitbull (Ft. Lil Jon)   \n",
       "2    Pitbull (Ft. Jamie Drastik)  305 In Paris by Pitbull (Ft. Jamie Drastik)   \n",
       "3                        Pitbull                     305 Til I Die by Pitbull   \n",
       "4        Los Rafas (Ft. Pitbull)       3bkzo RIMIX by Los Rafas (Ft. Pitbull)   \n",
       "\n",
       "        id          title               title_with_featured  \\\n",
       "0   654086          11:59       11:59 (Ft. Vein (Producer))   \n",
       "1    37215     305 Anthem          305 Anthem (Ft. Lil Jon)   \n",
       "2  9071119   305 In Paris  305 In Paris (Ft. Jamie Drastik)   \n",
       "3  8988638  305 Til I Die                     305 Til I Die   \n",
       "4  3972879    3bkzo RIMIX         3bkzo RIMIX (Ft. Pitbull)   \n",
       "\n",
       "                                               url  \\\n",
       "0          https://genius.com/Pitbull-11-59-lyrics   \n",
       "1     https://genius.com/Pitbull-305-anthem-lyrics   \n",
       "2   https://genius.com/Pitbull-305-in-paris-lyrics   \n",
       "3  https://genius.com/Pitbull-305-til-i-die-lyrics   \n",
       "4  https://genius.com/Los-rafas-3bkzo-rimix-lyrics   \n",
       "\n",
       "                                    featured_artists  artist  \\\n",
       "0  [{'api_path': '/artists/7076', 'header_image_u...       1   \n",
       "1  [{'api_path': '/artists/107', 'header_image_ur...       1   \n",
       "2  [{'api_path': '/artists/26923', 'header_image_...       1   \n",
       "3                                                 []       1   \n",
       "4  [{'api_path': '/artists/628', 'header_image_ur...       1   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  11 Contributors11:59 Lyrics[Intro: Pitbull & V...   \n",
       "1  17 Contributors305 Anthem Lyrics[Intro: Pitbul...   \n",
       "2  1 Contributor305 In Paris Lyrics[Intro]\\nWe're...   \n",
       "3  1 Contributor305 Til I Die Lyrics[Intro]\\nIntr...   \n",
       "4  1 Contributor3bkzo RIMIX Lyrics[INTRO]\\nMateín...   \n",
       "\n",
       "                                      cleaned_lyrics  \\\n",
       "0  \\nWe shut down Times Square, extra extra read ...   \n",
       "1  \\nMan I've been on the grind\\n1 in the head, 1...   \n",
       "2  [Chorus: Pitbull]\\nFlow so hard that chico get...   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "                                clean_cleaned_lyrics  \n",
       "0  ['', 'pitbull', 'vein', 'vein', 'someone', 'po...  \n",
       "1  ['', 'anthem', 'pitbull', 'lil', 'chico', 'pit...  \n",
       "2  ['', 'paris', 'gonna', 'skate', 'one', 'song',...  \n",
       "3  ['', 'til', 'die', 'mr', 'three', 'yeah', 'yea...  \n",
       "4  ['', 'rimix', 'mateín', 'checho', 'representan...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e23a48bc-5236-435b-b306-02b9b40bf44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1258, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245d552-0dd6-43ba-8998-9d4f8f12d6d8",
   "metadata": {},
   "source": [
    "## Train-Test-Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cea58-5a6b-4148-bd54-29a7d555641d",
   "metadata": {},
   "source": [
    "dale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60bb03a7-c256-4ede-a4ba-fabe253808e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined['clean_cleaned_lyrics']\n",
    "y = combined['artist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f37dc58d-7cb6-47dd-9d8e-f5c1ea60e425",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44321a-80b0-4a46-ae04-a0525644e6e6",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab294156-744c-44af-b9c7-53fcbb17d043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333863275039745"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_accuracy = combined['artist'].value_counts(normalize=True).max()\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e1aee-a0a6-4696-b187-fa0a5be4a9a3",
   "metadata": {},
   "source": [
    "The baseline model predicts Pitbull more because he has more songs, and is accurate around 53% of the time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336be6b-84af-4b36-ad46-b6cfb7658044",
   "metadata": {},
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c101a5a9-fd7f-4061-9d4c-9801db95ccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=['english', 'spanish'])),\n",
    "    ('lr', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9c2e53-c4f0-42b6-be5a-9b89c367bf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8227500328914273"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipeline, X_train, y_train, cv=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2700718f-21ce-4d5d-8792-93566b5223c3",
   "metadata": {},
   "source": [
    "The cross validation score is fairly high (0.82), meaning it should do well on new data. The model so far is able to predict whether or not it's Pitbull of 82% of the songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9353790d-e70b-43c4-90a5-561a0409217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9784090909090909\n",
      "Test: 0.8809523809523809\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {pipeline.score(X_train, y_train)}')\n",
    "print(f'Test: {pipeline.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c498ed-5275-485a-a786-456ecc0eb129",
   "metadata": {},
   "source": [
    "The model has a high accuracy score on both the training set (97.8%) and the test set (88%), which shows that the model is performing well on both datasets but seems to be overfitting some on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a171c-fe25-426d-b693-48513b52ab01",
   "metadata": {},
   "source": [
    "I chose to not continue with the Logistic Regression model to spend more time with RNN and BERT, as I believed those models would be more fitting for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dc49a7-b14e-43e8-b161-5c219b14c135",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0318b1-b147-4731-ae53-b2da1d8aa826",
   "metadata": {},
   "source": [
    "*My computer gets very loud running these :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42a52f23-f93e-4726-a302-d96712e36dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 02:34:31.975657: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-10 02:34:31.977528: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-10 02:34:31.979049: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-10 02:34:32.511082: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-10 02:34:32.513902: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-10 02:34:32.517548: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-06-10 02:34:33.919720: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-10 02:34:33.923147: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-10 02:34:33.924707: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.5447"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 02:35:10.105867: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-06-10 02:35:10.107542: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-06-10 02:35:10.108743: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 40s 1s/step - loss: 0.6902 - accuracy: 0.5447 - val_loss: 0.6769 - val_accuracy: 0.5714\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 37s 1s/step - loss: 0.6235 - accuracy: 0.7376 - val_loss: 0.5871 - val_accuracy: 0.7659\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 37s 1s/step - loss: 0.2442 - accuracy: 0.9334 - val_loss: 0.5156 - val_accuracy: 0.7976\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 42s 1s/step - loss: 0.0863 - accuracy: 0.9781 - val_loss: 0.5968 - val_accuracy: 0.7778\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 35s 1s/step - loss: 0.0644 - accuracy: 0.9841 - val_loss: 0.6625 - val_accuracy: 0.7302\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 35s 1s/step - loss: 0.0955 - accuracy: 0.9801 - val_loss: 0.7734 - val_accuracy: 0.7619\n",
      "8/8 [==============================] - 2s 246ms/step - loss: 0.5156 - accuracy: 0.7976\n",
      "Test Loss: 0.5156355500221252\n",
      "Test Accuracy: 0.7976190447807312\n"
     ]
    }
   ],
   "source": [
    "# Set random seed \n",
    "tf.random.set_seed(10)\n",
    "\n",
    "# Prepare the data\n",
    "lyrics = combined['clean_cleaned_lyrics'].values\n",
    "labels = combined['artist'].values\n",
    "\n",
    "# Tokenize lyrics \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lyrics)\n",
    "sequences = tokenizer.texts_to_sequences(lyrics)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "padded_sequences = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "embedding_dim = 100\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a358b81-46f5-4c0e-9aa8-8677be7dec38",
   "metadata": {},
   "source": [
    "The RNN varies in accuracy, between 70% to over 90% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd809216-abbd-4d91-b6fc-6faea8fa9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to try in app\n",
    "model.save('../RNN/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9b517-b20a-4dbe-96a4-8bbde2f51976",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21e729-fdc2-453d-bf64-f140cc976a98",
   "metadata": {},
   "source": [
    "![why](../Images/whyrunning.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50096191-d7ae-4a1f-9aa6-bcb8b55b1bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/danner/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/3: 100%|█████████████████████████████| 63/63 [19:40<00:00, 18.75s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5042576262402156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████| 63/63 [18:13<00:00, 17.36s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.3594796810121763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████████████████████████| 63/63 [20:03<00:00, 19.11s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.26615404964439454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████| 16/16 [01:15<00:00,  4.70s/batches]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9166666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#https://discuss.huggingface.co/t/not-using-gpu-although-it-is-specified/14746/2\n",
    "# Set the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the pre-trained BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(combined['clean_cleaned_lyrics'], combined['artist'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text\n",
    "max_length = 128 \n",
    "train_tokens = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_tokens = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=max_length,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_tokens['input_ids'], train_tokens['attention_mask'], torch.tensor(train_labels.tolist()))\n",
    "test_dataset = TensorDataset(test_tokens['input_ids'], test_tokens['attention_mask'], torch.tensor(test_labels.tolist()))\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16  \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, sampler=SequentialSampler(test_dataset))\n",
    "\n",
    "# Load the pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2) \n",
    "model.to(device)\n",
    "\n",
    "# Set the optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 3  \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batches'):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Average training loss: {average_loss}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "for batch in tqdm(test_dataloader, desc='Evaluating', unit='batches'):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
    "    true_labels.extend(inputs['labels'].tolist())\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = np.array(predictions) == np.array(true_labels)\n",
    "accuracy = np.sum(correct_predictions) / len(correct_predictions)\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ac4636-5083-4808-a03c-e274c06658cd",
   "metadata": {},
   "source": [
    "#### Save model for Streamlit app\n",
    "\n",
    "I chose to use this model as it's results have been more contistently high (over 91%) and the way the BERT model works seems better fit for analyzing the generated lyrics, as it takes into account the relationships between words on either sides of a word. The model works significantly better than baseline (53%) and has worked well in the streamlit app. \n",
    "\n",
    "Also, I have spent too much time loading this thing to not use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bf28367-2c16-4480-a856-19498efec934",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('../BERT/bert_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee50880-ca4f-4dce-982c-b69999c371c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../BERT/bert_tokenizer/tokenizer_config.json',\n",
       " '../BERT/bert_tokenizer/special_tokens_map.json',\n",
       " '../BERT/bert_tokenizer/vocab.txt',\n",
       " '../BERT/bert_tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('../BERT/bert_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df046a3-518a-47d2-984d-c4a988c1da9e",
   "metadata": {},
   "source": [
    "![count](../Images/count.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5a9a9a-acaa-4ecb-92f6-9a912eb07559",
   "metadata": {},
   "source": [
    "I have been listening to Pitbull for hours on end while doing this project. For research.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
